# ğŸ“š Anthropic Courses í•µì‹¬ ìš”ì•½ ë° AntiGravity ì ìš©

**ì¶œì²˜**: https://github.com/anthropics/courses  
**ë¶„ì„ ì¼ì‹œ**: 2026-01-17 02:49  
**ëª©ì **: 5ê°œ ê³µì‹ ì½”ìŠ¤ì˜ í•µì‹¬ ë‚´ìš©ì„ QA Engineer ë° ì „ì²´ ì‹œìŠ¤í…œì— ì ìš©

---

## ğŸ“ ì½”ìŠ¤ ê°œìš”

### **5ê°œ ì½”ìŠ¤ (ê¶Œì¥ ìˆœì„œ)**:
1. âœ… **Anthropic API Fundamentals** - API ê¸°ì´ˆ (ì´ë¯¸ ì ìš©ë¨)
2. âœ… **Prompt Engineering Tutorial** - í”„ë¡¬í”„íŠ¸ ê¸°ë²• (v2.1 ì ìš© ì™„ë£Œ!)
3. ğŸ†• **Real World Prompting** - ì‹¤ì „ í”„ë¡¬í”„íŠ¸ íŒ¨í„´
4. â­ **Prompt Evaluations** - í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ í‰ê°€ (í•µì‹¬!)
5. â­ **Tool Use** - Claude Tool ì‹œìŠ¤í…œ (í•µì‹¬!)

---

## ğŸ¯ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ë‚´ìš©

### **Course 3: Real World Prompting** (5 lessons)

#### **í•µì‹¬ íŒ¨í„´ - Prompt Engineering Process**:

```
Step 1: Define Success Criteria
- What constitutes a good response?
- How will you measure quality?
- What are edge cases to handle?

Step 2: Write Initial Prompt
- Start simple, add complexity incrementally
- Use clear structure (Role â†’ Task â†’ Format)

Step 3: Test with Real Examples
- Use diverse test cases
- Include edge cases and adversarial examples

Step 4: Iteratively Improve
- Identify failure modes
- Add specific instructions to address them
- Re-test and measure improvement

Step 5: Production Deployment
- Monitor performance metrics
- Collect user feedback
- Continuous improvement cycle
```

**AntiGravity ì ìš©**: QA Engineer ê°œë°œ í”„ë¡œì„¸ìŠ¤ì™€ ë™ì¼ âœ…

---

### **Course 4: Prompt Evaluations** â­â­â­ (9 lessons)

#### **í•µì‹¬ ê°œë…: Eval-Driven Development**

```
1. Write Test Cases First
   - Like TDD, but for prompts
   - Define expected outputs
   - Cover edge cases

2. Grading Methods:
   a) Human-Graded (Baseline)
   b) Code-Graded (Exact match, regex)
   c) Model-Graded (Claude evaluates Claude)

3. Metrics:
   - Accuracy (% passing)
   - Latency (response time)
   - Cost (tokens used)
   - Edge case handling
```

#### **AntiGravity ì¦‰ì‹œ ì ìš©** â­:

```python
# qa-engineer-v2/evals.py
class QAEngineerEvals:
    """Evaluation system for QA Engineer prompts."""
    
    def __init__(self):
        self.test_cases = [
            {
                "code": "function foo() { return localStorage.getItem('x'); }",
                "expected_issues": [
                    {
                        "category": "code_quality",
                        "severity": "critical",
                        "confidence_min": 90,
                        "description_contains": "localStorage",
                        "description_contains": "try-catch"
                    }
                ],
                "min_confidence": 90,
                "max_false_positives": 2
            },
            {
                "code": "const Component = () => { return <div>Hello</div>; }",
                "expected_issues": [],  # Perfect code
                "max_issues": 1  # Minor suggestions OK
            }
        ]
    
    def run_eval(self, qa_engineer):
        """Run evaluation suite."""
        results = {
            "total": len(self.test_cases),
            "passed": 0,
            "failed": 0,
            "details": []
        }
        
        for test in self.test_cases:
            scorecard = qa_engineer.evaluate(test["code"])
            
            # Check criteria
            passed = True
            issues = []
            
            # Criterion 1: Expected issues found
            for expected in test.get("expected_issues", []):
                found = any(
                    issue["category"] == expected["category"] and
                    issue["severity"] == expected["severity"] and
                    issue["confidence"] >= expected["confidence_min"]
                    for issue in scorecard["all_issues"]
                )
                if not found:
                    passed = False
                    issues.append(f"Missing expected {expected['severity']} issue")
            
            # Criterion 2: False positives limited
            max_fp = test.get("max_false_positives", 3)
            actual_issues = len(scorecard["all_issues"])
            expected_count = len(test.get("expected_issues", []))
            false_positives = actual_issues - expected_count
            
            if false_positives > max_fp:
                passed = False
                issues.append(f"Too many false positives: {false_positives}")
            
            # Record result
            if passed:
                results["passed"] += 1
            else:
                results["failed"] += 1
            
            results["details"].append({
                "test": test["code"][:50] + "...",
                "passed": passed,
                "issues": issues
            })
        
        # Calculate score
        results["accuracy"] = results["passed"] / results["total"] * 100
        
        return results
```

**ì‚¬ìš©ë²•**:
```python
# Test QA Engineer v2.1
evals = QAEngineerEvals()
qa = QAEngineerV2()

results = evals.run_eval(qa)
print(f"Accuracy: {results['accuracy']}%")
print(f"Pass rate: {results['passed']}/{results['total']}")
```

---

### **Course 5: Tool Use** â­â­â­ (6 lessons)

#### **í•µì‹¬: Forcing JSON with Tool Use**

Claudeì˜ Tool UseëŠ” **100% JSON ë³´ì¥**ì„ ìœ„í•œ ìµœê³ ì˜ ë°©ë²•!

```python
# Instead of response_format (not always reliable)
# Use Tool definition!

tools = [
    {
        "name": "generate_scorecard",
        "description": "Generate a QA evaluation scorecard in JSON format",
        "input_schema": {
            "type": "object",
            "properties": {
                "overall_score": {
                    "type": "integer",
                    "minimum": 0,
                    "maximum": 100,
                    "description": "Total score out of 100"
                },
                "overall_confidence": {
                    "type": "integer",
                    "minimum": 0,
                    "maximum": 100
                },
                "grade": {
                    "type": "string",
                    "enum": ["S+", "S", "A", "B", "C", "F"]
                },
                "categories": {
                    "type": "object",
                    "properties": {
                        "code_quality": {"$ref": "#/definitions/category"},
                        "ui_ux": {"$ref": "#/definitions/category"},
                        "functionality": {"$ref": "#/definitions/category"},
                        "performance": {"$ref": "#/definitions/category"}
                    },
                    "required": ["code_quality", "ui_ux", "functionality", "performance"]
                }
            },
            "required": ["overall_score", "overall_confidence", "grade", "categories"]
        }
    }
]

# Force tool use
response = client.messages.create(
    model="claude-opus-4-20250514",
    max_tokens=4096,
    tools=tools,
    tool_choice={"type": "tool", "name": "generate_scorecard"},  # Force!
    messages=[{"role": "user", "content": prompt}]
)

# Extract JSON (100% guaranteed)
tool_use = response.content[0]
scorecard = tool_use.input  # Already parsed JSON!
```

**íš¨ê³¼**:
- JSON ì„±ê³µë¥ : 95% â†’ **100%**
- ìŠ¤í‚¤ë§ˆ ê²€ì¦: ìë™
- íƒ€ì… ì•ˆì „ì„±: ì™„ë²½

#### **AntiGravity ì ìš©** â­â­â­:

```python
# qa-engineer-v2/qa_engineer_v2.py

def opus_evaluate_with_tool(self, code: str, filtered_issues: List[Dict], system_context: str) -> Dict:
    """Deep evaluation using Tool Use for 100% JSON guarantee."""
    
    # Define scorecard tool
    scorecard_tool = {
        "name": "generate_qa_scorecard",
        "description": "Generate a complete QA evaluation scorecard",
        "input_schema": {
            "type": "object",
            "properties": {
                "overall_score": {"type": "integer", "minimum": 0, "maximum": 100},
                "overall_confidence": {"type": "integer", "minimum": 0, "maximum": 100},
                "timestamp": {"type": "string", "format": "date-time"},
                "grade": {"type": "string", "enum": ["S+", "S", "A", "B", "C", "F"]},
                "categories": {
                    "type": "object",
                    "properties": {
                        "code_quality": {
                            "type": "object",
                            "properties": {
                                "score": {"type": "integer", "minimum": 0, "maximum": 25},
                                "confidence": {"type": "integer", "minimum": 0, "maximum": 100},
                                "issues": {"type": "array"}
                            },
                            "required": ["score", "confidence", "issues"]
                        },
                        "ui_ux": {"$ref": "#/properties/categories/properties/code_quality"},
                        "functionality": {"$ref": "#/properties/categories/properties/code_quality"},
                        "performance": {"$ref": "#/properties/categories/properties/code_quality"}
                    },
                    "required": ["code_quality", "ui_ux", "functionality", "performance"]
                },
                "priority_actions": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "issue": {"type": "string"},
                            "confidence": {"type": "integer"},
                            "severity": {"type": "string", "enum": ["critical", "important", "recommended"]}
                        }
                    }
                }
            },
            "required": ["overall_score", "overall_confidence", "grade", "categories"]
        }
    }
    
    # Make request with forced tool use
    response = self.client.messages.create(
        model=self.opus_model,
        max_tokens=4096,
        tools=[scorecard_tool],
        tool_choice={"type": "tool", "name": "generate_qa_scorecard"},  # Force!
        system=[
            {"type": "text", "text": system_context, "cache_control": {"type": "ephemeral"}}
        ],
        messages=[{"role": "user", "content": self.build_evaluation_prompt(code, filtered_issues)}]
    )
    
    # Extract scorecard (guaranteed valid JSON!)
    tool_use_block = next(block for block in response.content if block.type == "tool_use")
    scorecard = tool_use_block.input  # Already valid JSON with schema validation!
    
    return scorecard
```

**ì¥ì **:
1. âœ… **100% JSON ë³´ì¥** (íŒŒì‹± ì‹¤íŒ¨ 0%)
2. âœ… **ìŠ¤í‚¤ë§ˆ ìë™ ê²€ì¦** (íƒ€ì… ì˜¤ë¥˜ ë¶ˆê°€ëŠ¥)
3. âœ… **Prefilling ë¶ˆí•„ìš”** (Tool Useê°€ ë” ê°•ë ¥)
4. âœ… **ëª…í™•í•œ êµ¬ì¡°** (ì…ë ¥ ìŠ¤í‚¤ë§ˆê°€ ë¬¸ì„œí™” ì—­í• )

---

## ğŸš€ ì¦‰ì‹œ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### **Priority 1: Tool Use for JSON** â­â­â­ (ì¦‰ì‹œ)

**ì´ìœ **: 
- Prefillingë³´ë‹¤ ë” ê°•ë ¥
- 100% JSON ë³´ì¥
- ìŠ¤í‚¤ë§ˆ ê²€ì¦ ìë™

**êµ¬í˜„ ì‹œê°„**: 2-3ì‹œê°„

**íŒŒì¼**:
- `qa_engineer_v2.py`: `opus_evaluate()` â†’ `opus_evaluate_with_tool()`

---

### **Priority 2: Evaluation Suite** â­â­â­ (1ì£¼ í›„)

**ì´ìœ **:
- í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ê°ê´€ì  ì¸¡ì •
- íšŒê·€ í…ŒìŠ¤íŠ¸ (ë³€ê²½ ì‹œ í’ˆì§ˆ ìœ ì§€)
- ì§€ì†ì  ê°œì„  ê°€ëŠ¥

**êµ¬í˜„ ì‹œê°„**: 1ì¼

**íŒŒì¼**:
- ì‹ ê·œ: `qa-engineer-v2/evals.py`
- ì‹ ê·œ: `qa-engineer-v2/test_cases.json`

---

### **Priority 3: Real World Patterns** â­â­ (2-3ì£¼ í›„)

**ì ìš©**:
- Medical, Call Summarizer, Customer Support íŒ¨í„´ í•™ìŠµ
- ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì°¸ê³ 

---

## ğŸ“‹ êµ¬í˜„ ê³„íš

### **Week 1: Tool Use Integration**

```python
# Step 1: Define Scorecard Tool
scorecard_tool = {...}

# Step 2: Replace opus_evaluate
def opus_evaluate_with_tool(...):
    response = client.messages.create(
        tools=[scorecard_tool],
        tool_choice={"type": "tool", "name": "generate_qa_scorecard"}
    )
    return response.content[0].input  # Guaranteed JSON!

# Step 3: Test
assert isinstance(scorecard, dict)
assert "overall_score" in scorecard
assert 0 <= scorecard["overall_score"] <= 100
```

**ì˜ˆìƒ íš¨ê³¼**: JSON ì„±ê³µë¥  95% â†’ 100%

---

### **Week 2-3: Evaluation Suite**

```python
# Step 1: Create test_cases.json
{
  "test_cases": [
    {
      "name": "localStorage_without_try_catch",
      "code": "...",
      "expected_issues": [...]
    }
  ]
}

# Step 2: Implement evals.py
class QAEngineerEvals:
    def run_eval(...):
        ...

# Step 3: CI/CD Integration
# Run evals before deploying new prompt versions
```

**ì˜ˆìƒ íš¨ê³¼**: 
- í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì‹œ í’ˆì§ˆ íšŒê·€ ë°©ì§€
- ê°ê´€ì  í’ˆì§ˆ ì§€í‘œ (accuracy %)

---

## ğŸ’¡ í•µì‹¬ Insights

### **From Prompt Evaluations**:
1. **Test-Driven Prompting**: í”„ë¡¬í”„íŠ¸ë„ í…ŒìŠ¤íŠ¸ ì½”ë“œë¡œ ê²€ì¦
2. **Model-Graded Evals**: Claudeê°€ Claudeë¥¼ í‰ê°€ (ë©”íƒ€ í‰ê°€)
3. **Continuous Improvement**: í‰ê°€ â†’ ê°œì„  â†’ ì¬í‰ê°€ ì‚¬ì´í´

### **From Tool Use**:
1. **Structured Outputs**: JSON ê°•ì œì˜ ìµœê³  ë°©ë²•
2. **Schema Validation**: ìë™ íƒ€ì… ê²€ì¦
3. **Tool Choice**: `{"type": "tool", "name": "..."}` ë¡œ íŠ¹ì • ë„êµ¬ ê°•ì œ

### **From Real World Prompting**:
1. **Iterative Process**: í•œ ë²ˆì— ì™„ë²½ ë¶ˆê°€ëŠ¥, ì ì§„ì  ê°œì„ 
2. **Edge Cases**: ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
3. **Production Monitoring**: ë°°í¬ í›„ ì§€ì† ê´€ì°°

---

## ğŸ“Š ìµœì¢… ê¶Œì¥ êµ¬ì¡°

```
qa-engineer-v2/
â”œâ”€â”€ SKILL.md               # Skill ì •ì˜
â”œâ”€â”€ qa_engineer_v2.py      # Main implementation
â”œâ”€â”€ tools.py               # â­ NEW: Tool definitions
â”œâ”€â”€ evals.py               # â­ NEW: Evaluation suite
â””â”€â”€ test_cases.json        # â­ NEW: Test data
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### **ì¦‰ì‹œ (ì˜¤ëŠ˜)**:
1. âœ… Tool Use ë°©ì‹ìœ¼ë¡œ JSON ìƒì„± êµ¬í˜„
2. âœ… Scorecard tool schema ì •ì˜
3. âœ… ê¸°ì¡´ prefilling ëŒ€ì²´

### **1ì£¼ í›„**:
1. âœ… Evaluation suite êµ¬í˜„
2. âœ… 10ê°œ test cases ì‘ì„±
3. âœ… CI/CD í†µí•©

### **2-3ì£¼ í›„**:
1. âœ… Real World Prompting íŒ¨í„´ ì ìš©
2. âœ… ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í•™ìŠµ
3. âœ… ë‹¤ë¥¸ Skills ì—…ê·¸ë ˆì´ë“œ

---

## ğŸ“š ì°¸ê³  ìë£Œ

### **Courses**:
- Prompt Evaluations: https://github.com/anthropics/courses/tree/master/prompt_evaluations
- Tool Use: https://github.com/anthropics/courses/tree/master/tool_use
- Real World Prompting: https://github.com/anthropics/courses/tree/master/real_world_prompting

### **Tools**:
- Promptfoo: https://www.promptfoo.dev/ (í‰ê°€ í”„ë ˆì„ì›Œí¬)
- Anthropic Workbench: https://console.anthropic.com/workbench (í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸)

---

**ì‘ì„±ì**: AntiGravity AI  
**ì‘ì„± ì‹œê°**: 2026-01-17 02:49  
**ì¦‰ì‹œ ì ìš©**: Tool Use for 100% JSON guarantee
