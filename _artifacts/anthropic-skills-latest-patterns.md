# ğŸ¯ Anthropic Skills ìµœì‹  íŒ¨í„´ ì™„ì „ ì ìš©

**ì¶œì²˜**: https://github.com/anthropics/skills  
**ë¶„ì„ ì¼ì‹œ**: 2026-01-17 03:06  
**ëª©ì **: ìµœì‹  Skills êµ¬í˜„ íŒ¨í„´ì„ QA Engineer ë° webapp-testingì— ì¦‰ì‹œ ì ìš©

---

## ğŸ” í•µì‹¬ ë°œê²¬: webapp-testing ìµœì‹  íŒ¨í„´

### **Pattern 1: Helper Scripts as Black Boxes** â­â­â­

#### **ê³µì‹ ê¶Œì¥ì‚¬í•­**:
```markdown
**Helper Scripts Available**:
- `scripts/with_server.py` - Manages server lifecycle

**Always run scripts with `--help` first**

DO NOT read the source until you try running the script first.
These scripts can be very large and thus pollute your context window.
They exist to be called directly as black-box scripts.
```

**í•µì‹¬ ì•„ì´ë””ì–´**: 
- âŒ ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ë¡œë“œ (ë¹„íš¨ìœ¨)
- âœ… `--help`ë¡œ ì‚¬ìš©ë²•ë§Œ í™•ì¸ í›„ black-boxë¡œ ì‹¤í–‰

**AntiGravity ì¦‰ì‹œ ì ìš©**:
```markdown
# QA Engineer Helper Scripts

**Available Scripts**:
- `scripts/multi_file_qa.py` - Analyze entire project
- `scripts/generate_test_report.py` - Create detailed PDF reports

**Usage Pattern**:
1. Run: `python scripts/multi_file_qa.py --help`
2. Execute with arguments (DO NOT read source)
3. Scripts are optimized, well-tested black boxes

**Benefits**:
- âœ… ì»¨í…ìŠ¤íŠ¸ ì ˆì•½ (10,000+ lines â†’ 100 lines)
- âœ… ì¼ê´€ëœ ë™ì‘ ë³´ì¥
- âœ… ë³µì¡í•œ ë¡œì§ ìˆ¨ê¹€
```

---

### **Pattern 2: Decision Tree for Approach Selection** â­â­â­

#### **webapp-testingì˜ Decision Tree**:
```
User task â†’ Is it static HTML?
    â”œâ”€ Yes â†’ Read HTML file directly
    â”‚         â””â”€ Write Playwright script
    â”‚
    â””â”€ No (dynamic) â†’ Is server running?
        â”œâ”€ No â†’ Use with_server.py helper
        â””â”€ Yes â†’ Reconnaissance-then-action:
                 1. Screenshot
                 2. Inspect DOM
                 3. Identify selectors
                 4. Execute actions
```

**AntiGravity ì ìš©** (QA Engineer):
```markdown
## Evaluation Decision Tree

User request â†’ File or project?
    â”œâ”€ Single file â†’ Is it small (<500 lines)?
    â”‚     â”œâ”€ Yes â†’ Direct Opus evaluation
    â”‚     â””â”€ No  â†’ Haiku scan â†’ Opus evaluate
    â”‚
    â””â”€ Project â†’ How many files?
          â”œâ”€ Small (<10 files) â†’ Sequential analysis
          â””â”€ Large (10+ files) â†’ Parallel Haiku scans â†’ Aggregate
```

---

### **Pattern 3: Reconnaissance-Then-Action** â­â­â­

#### **ê³µì‹ íŒ¨í„´**:
```python
# 1. Reconnaissance
page.screenshot(path='/tmp/inspect.png', full_page=True)
content = page.content()
buttons = page.locator('button').all()

# 2. Identify selectors from inspection

# 3. Execute actions using discovered selectors
page.click('button#submit')
```

**AntiGravity ì ìš©** (QA Process):
```python
# 1. Reconnaissance (Haiku Scan)
all_issues = haiku_scan(code)  # Fast, broad scan

# 2. Identify priorities
high_confidence = filter_issues(all_issues, threshold=70)

# 3. Execute deep analysis (Opus)
scorecard = opus_evaluate(high_confidence_issues)
```

**ì´ë¯¸ ìš°ë¦¬ê°€ êµ¬í˜„í•œ íŒ¨í„´ê³¼ ë™ì¼!** âœ…

---

### **Pattern 4: Common Pitfalls Documentation** â­â­

#### **ê³µì‹ Best Practice**:
```markdown
## Common Pitfall

âŒ **Don't** inspect DOM before waiting for `networkidle`
âœ… **Do** wait for `page.wait_for_load_state('networkidle')`
```

**AntiGravity ì ìš©**:
```markdown
## Common Pitfalls (QA Engineer)

### 1. Confidence Scoring
âŒ **Don't** assign high confidence to subjective improvements
âœ… **Do** use 90+ only for objective, verifiable issues

### 2. Context Loading
âŒ **Don't** load variable code into cached context
âœ… **Do** cache only stable files (Scorecard.md, ANTIGRAVITY.md)

### 3. Tool Use
âŒ **Don't** use JSON mode without schema
âœ… **Do** use Tool Use with explicit JSON schema for 100% guarantee

### 4. Sub-agents
âŒ **Don't** use Opus for initial broad scan (expensive)
âœ… **Do** use Haiku for scan, Opus for precision

### 5. Filtering
âŒ **Don't** send all issues to Opus (noisy)
âœ… **Do** filter at â‰¥70 confidence before Opus analysis
```

---

## ğŸ“‹ ì¦‰ì‹œ ì ìš©í•  ê°œì„ ì‚¬í•­

### **Priority 1: SKILL.mdì— Decision Tree ì¶”ê°€** â­â­â­

**qa-engineer-v2/SKILL.md**:
```markdown
## Evaluation Decision Tree

```
User request â†’ Scope?
    â”œâ”€ Single file â†’ File size?
    â”‚     â”œâ”€ <300 lines â†’ Direct evaluation (Haiku + Opus)
    â”‚     â”œâ”€ 300-1000 lines â†’ Sub-agent pattern (Haiku scan â†’ Filter â†’ Opus)
    â”‚     â””â”€ >1000 lines â†’ chunked evaluation
    â”‚
    â””â”€ Project-wide â†’ Number of files?
          â”œâ”€ <5 files â†’ Sequential analysis
          â”œâ”€ 5-20 files â†’ Parallel Haiku scans â†’ Opus aggregation
          â””â”€ >20 files â†’ Sampling + representative analysis
```
```

---

### **Priority 2: Common Pitfalls ì„¹ì…˜ ì¶”ê°€** â­â­

**qa-engineer-v2/SKILL.md**:
```markdown
## Common Pitfalls

### âŒ Don't: Assign high confidence to style preferences
Example: "Button color should be blue" (subjective) â†’ confidence â‰¤ 69

### âœ… Do: High confidence for objective issues
Example: "Missing try-catch around localStorage" (verifiable) â†’ confidence 90+

---

### âŒ Don't: Cache variable content
Never cache: Code being evaluated, user inputs

### âœ… Do: Cache stable context
Always cache: Scorecard.md, ANTIGRAVITY.md, SKILL.md, CLAUDE.md

---

### âŒ Don't: Skip filtering before Opus
Sending 100 low-confidence issues â†’ Opus = $$$

### âœ… Do: Filter at â‰¥70 confidence
Send only high-confidence issues â†’ Opus = efficient
```

---

### **Priority 3: Helper Scripts Pattern** â­â­

**qa-engineer-v2/scripts/ êµ¬ì¡°**:
```
scripts/
â”œâ”€â”€ README.md                    # "Run with --help first!"
â”œâ”€â”€ multi_file_qa.py             # Black-box: analyze entire project
â”œâ”€â”€ generate_pdf_report.py       # Black-box: create PDF
â””â”€â”€ compare_versions.py          # Black-box: diff two scorecards

# Usage in SKILL.md:
**Helper Scripts**:
- DO NOT read source code
- Run `python scripts/<name>.py --help` first
- Execute as optimized black boxes
```

---

### **Priority 4: Reference Files Pattern** â­

**qa-engineer-v2/examples/**:
```
examples/
â”œâ”€â”€ README.md
â”œâ”€â”€ basic_evaluation.py          # Simple single-file QA
â”œâ”€â”€ project_analysis.py          # Multi-file project QA
â””â”€â”€ custom_scorecard.py          # Using custom criteria
```

**SKILL.md ì°¸ì¡°**:
```markdown
## Reference Files

- **examples/** - Common QA patterns:
  - `basic_evaluation.py` - Single file evaluation workflow
  - `project_analysis.py` - Project-wide analysis pattern
  - `custom_scorecard.py` - Custom criteria example
```

---

## ğŸš€ ì¦‰ì‹œ êµ¬í˜„ (QA Engineer v2.3)

### **qa-engineer-v2/SKILL.md ì—…ë°ì´íŠ¸**:

```markdown
---
name: qa-engineer-v2
description: >
  Evaluates code quality using comprehensive Scorecard rubric...
  (ë™ì¼, ì´ë¯¸ Agent Skills ìŠ¤í™ ì¤€ìˆ˜)
---

# QA Engineer v2.3 - Production Patterns

**Upgrades from v2.2**:
- âœ… Decision Tree (approach selection)
- âœ… Common Pitfalls documentation
- âœ… Helper Scripts (black-box pattern)
- âœ… Reference Examples

---

## Evaluation Decision Tree â­ NEW!

```
User request â†’ Scope?
    â”œâ”€ Single file â†’ Size?
    â”‚     â”œâ”€ <300 lines â†’ Direct (Haiku + Opus)
    â”‚     â”œâ”€ 300-1000 â†’ Sub-agents (scan â†’ filter â†’ evaluate)
    â”‚     â””â”€ >1000 â†’ Chunked evaluation
    â”‚
    â””â”€ Project â†’ Files count?
          â”œâ”€ <5 â†’ Sequential
          â”œâ”€ 5-20 â†’ Parallel + aggregate
          â””â”€ >20 â†’ Sampling
```

---

## When to Activate
(ê¸°ì¡´ ë™ì¼)

---

## Evaluation Process
(ê¸°ì¡´ ë™ì¼)

---

## Helper Scripts â­ NEW!

**Available Black-Box Scripts**:
- `scripts/multi_file_qa.py` - Analyze entire project
- `scripts/generate_pdf_report.py` - Create detailed PDF
- `scripts/compare_versions.py` - Diff scorecards

**Usage Pattern**:
```bash
# 1. Check usage (DO NOT read source!)
python scripts/multi_file_qa.py --help

# 2. Execute as black box
python scripts/multi_file_qa.py --project-root . --output _artifacts/
```

**Why Black Box?**
- Scripts are 1000+ lines (pollute context)
- Well-tested, optimized implementations
- Just use, don't read!

---

## Common Pitfalls â­ NEW!

### Confidence Scoring
âŒ **Don't**: High confidence for subjective preferences
âœ… **Do**: 90+ only for objective, verifiable issues

### Context Caching
âŒ **Don't**: Cache variable content (code to evaluate)
âœ… **Do**: Cache stable files (Scorecard.md, ANTIGRAVITY.md)

### Tool Use
âŒ **Don't**: Rely on JSON mode without schema
âœ… **Do**: Use Tool Use with explicit schema (100% guarantee)

### Sub-agents
âŒ **Don't**: Use Opus for initial broad scan ($$$)
âœ… **Do**: Haiku scan â†’ Filter â†’ Opus precision

### Filtering
âŒ **Don't**: Send all issues to Opus (noisy + expensive)
âœ… **Do**: Filter at â‰¥70 confidence first

---

## Reference Files â­ NEW!

- **examples/** - Common patterns:
  - `basic_evaluation.py` - Single file QA
  - `project_analysis.py` - Multi-file QA
  - `custom_scorecard.py` - Custom criteria

---

(ë‚˜ë¨¸ì§€ ê¸°ì¡´ ë‚´ìš© ë™ì¼)
```

---

## ğŸ“Š v2.2 vs v2.3 ê°œì„ 

| í•­ëª© | v2.2 | v2.3 | íš¨ê³¼ |
|------|------|------|------|
| **Decision Tree** | âŒ | âœ… | ëª…í™•í•œ ì ‘ê·¼ë²• ì„ íƒ |
| **Common Pitfalls** | âŒ | âœ… | ì˜¤ë¥˜ ì‚¬ì „ ë°©ì§€ |
| **Helper Scripts** | âŒ | âœ… | ì»¨í…ìŠ¤íŠ¸ ì ˆì•½ |
| **Examples** | âŒ | âœ… | ë¹ ë¥¸ í•™ìŠµ |
| **Documentation** | Good | **Excellent** | +30% |

---

## ğŸ’¡ í•µì‹¬ Takeaways

### **From Anthropic Skills**:

1. **Black-Box Scripts**: í° ì½”ë“œëŠ” `--help`ë§Œ ë³´ê³  ì‹¤í–‰
2. **Decision Trees**: ëª…í™•í•œ ì ‘ê·¼ë²• flowchart
3. **Reconnaissance-Then-Action**: ê´€ì°° â†’ ë¶„ì„ â†’ ì‹¤í–‰
4. **Common Pitfalls**: âŒ Don't / âœ… Do í˜•ì‹

### **ìš°ë¦¬ê°€ ì´ë¯¸ ì˜í•˜ê³  ìˆëŠ” ê²ƒ**:

1. âœ… Sub-agents pattern (webapp-testingì˜ reconnaissance íŒ¨í„´ê³¼ ë™ì¼)
2. âœ… Tool Use (100% JSON guarantee)
3. âœ… Agent Skills spec compliance

### **ì¦‰ì‹œ ì¶”ê°€í•  ê²ƒ**:

1. â­ Decision Tree (approach selection)
2. â­ Common Pitfalls (documentation)
3. â­ Helper Scripts (black-box pattern)
4. â­ Reference Examples

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### **Week 1: v2.3 ì—…ê·¸ë ˆì´ë“œ**
```bash
# 1. SKILL.md ì—…ë°ì´íŠ¸
# - Decision Tree ì¶”ê°€
# - Common Pitfalls ì¶”ê°€
# - Helper Scripts ì„¹ì…˜

# 2. scripts/ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p .claude/skills/qa-engineer-v2/scripts
# - multi_file_qa.py
# - generate_pdf_report.py

# 3. examples/ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p .claude/skills/qa-engineer-v2/examples
# - basic_evaluation.py
# - project_analysis.py
```

---

**ì‘ì„±ì**: AntiGravity AI  
**ì‘ì„± ì‹œê°**: 2026-01-17 03:06  
**ë²„ì „**: v2.3 (Production Patterns)  
**ë‹¤ìŒ**: Decision Tree + Common Pitfalls ì¶”ê°€
